#!/usr/bin/python3

import re

from difflib import unified_diff
from pathlib import Path
from sys import exit
from shutil import copy

import codec_mazovia

PROJECTS = (
    "append",
    "assign",
    "blocek",
    "choice",
    "compute",
    "ctmouse",
    "diskcopy",
    "display",
    "doslfn",
    "dosshell",
    "dosutil",
    "edict",
    "edlin",
    "exe2bin",
    "fc",
    "fdhelper",
    "fdi",
    "fdimples",
    "fdisk",
    "fdi-x86",
    "fdnet",
    "fdnpkg",
    "fdpkg",
    "fdshell",
    "fdtui",
    "find",
    "flashrom",
    "freecom",
    "gcdrom",
    "help-legacy",
    "htmlhelp",
    "label",
    "localize",
    "md5sum",
    "mem",
    "mirror",
    "mkeyb",
    "mode",
    "more",
    "move",
    "nlsfunc",
    "packages",
    "password",
    "pause",
    "pdtree",
    "pgme",
    "pkgtools",
    "runtime",
    "slicer",
    "sort",
    "stamp",
    "syslnx",
    "tee",
    "test.out",
    "trch",
    "tree",
    "usbdos",
    "v8power",
    "vmsmount",
    "wget",
    "xcopy",
    "xdel",
)

LANGUAGES = {
    "en": "CP437",
    "br": "CP850",  # Only CTMOUSE uses 'br' for Brazilian Portuguese
    "cz": "CP852",  # Czech language code is really 'cs' (ISO 639-1)
    "da": "CP850",
    "de": "CP850",
    "dk": "CP850",  # Actually we really should be using 'da' not 'dk'
    "eo": None,     # Esperanto??
    "es": "CP850",
    "eu": None,     # Basque maybe CP850?
    "fi": "CP850",
    "fr": "CP850",
    "hu": "CP852",
    "is": "CP850",  # Icelandic
    "it": "CP850",
    "ja": None,     # Unknown target encoding, doesn't it need DBCS support everywhere?
    "la": None,     # Latin??
    "lv": "CP775",
    "nl": "CP850",
    "no": "CP850",
    "pl": "mazovia", # our local implementation
    "pt": "CP850",
    "ptBR": "CP850", # name will be truncated in _output
    "rs": "CP852",
    "ru": "CP866",
    "si": "CP852",  # Slovene (but only fdnpkg uses it, others use 'sl')
    "sk": None,     # Unknown (but only used by htmlhelp)
    "sl": "CP852",
    "sv": "CP850",
    "tr": "CP857",
    "ua": "CP866",
}


output = Path("_output")

for p in PROJECTS:

    print("Processing '%s'" % p)

    for d in ["nls", "help"]:

        print("  Directory '%s'" % d)

        indir = Path(p) / d
        outdir = output / p / d
        outdir.mkdir(parents=True, exist_ok=True)

        missing = []

        for lng in LANGUAGES:
            vrbsrc = indir / (p + "." + lng)
            utfsrc = indir / (p + "." + lng + ".UTF-8")

            if lng == 'ptBR':
                outtgt = outdir / (p + ".ptb")
            else:
                outtgt = outdir / (p + "." + lng)

            if utfsrc.is_file():
                txt = utfsrc.read_text(encoding="UTF-8")

                hdr = """\
# This file is autogenerated, any updates will be lost. Please visit
# https://github.com/shidel/fd-nls to update the UTF-8 version of it.
#
"""
                if d == 'help':
                    hdr = hdr.replace('#', ';')

                content = hdr + txt

                try:
                    # Would like to do this, but newline only supported on Python 3.10+
                    # outtgt.write_text(content, encoding=LANGUAGES[lng], newline='\r\n')

                    # So have to do this instead
                    outtgt.write_text(content.replace('\n', '\r\n'), encoding=LANGUAGES[lng])

                except UnicodeEncodeError as e:
                    print("    Error: Conversion for '%s' failed" % lng)
                    print("           (probably invalid character for target codepage)")
                    print("           ('%s')" % e)
                    exit(1)

                # If we have both UTF-8 and encoded source check to see if they match
                if vrbsrc.is_file():
                    vrb = vrbsrc.read_text(encoding=LANGUAGES[lng])
                    if vrb != txt:
                        print(
                            "    Error: Supplied encoded text for '%s' does not match UTF-8" % lng
                        )

                        txt_list = txt.split('\n')
                        vrb_list = vrb.split('\n')

                        diff = unified_diff(txt_list, vrb_list, lineterm='\n')
                        print('\n'.join(diff))
                        exit(1)


            elif vrbsrc.is_file():
                # Check for lost encoding i.e. all non 7bit ASCII Have been
                # replaced, so we look for ? characters in the middle of a
                # word. Notably the file will be readable as UTF-8 text, when
                # it's most unlikely that it should not (other than English).
                # It's not perfect but will catch some obvious errors.
                try:
                    txt = vrbsrc.read_text()
                    x = re.search(r"[a-zA-Z]\?[a-zA-Z]", txt)
                    if x:
                        print(
                            "    Error: Encoding for '%s' has suspect characters" % lng
                        )
                        print(x.string)
                        exit(1)
                except UnicodeDecodeError:
                    pass
                copy(vrbsrc, outdir)

            else:
                missing += [lng,]

        if missing:
            print("    Warning: No translation for %s" % missing)
